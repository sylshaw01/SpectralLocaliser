{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D SSH Model with Variable Kappa Analysis\n",
    "\n",
    "This notebook analyzes the 1dSSH model data with varying spectral localizer coupling (kappa) and disorder parameters.\n",
    "\n",
    "**Outputs:**\n",
    "- 3x2 grid plots comparing Hamiltonian and Spectral Localizer (DOS, eigenvalue index, IPR)\n",
    "- r/z statistics vs disorder (multiple kappa values as lines)\n",
    "- r/z statistics vs kappa (multiple disorder levels as lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports and Constants\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "\n",
    "# Plotting constants (matching analysis/3dAnderson-mobility-analysis.py)\n",
    "COLORS = {'H': 'blue', 'SL': 'orange'}\n",
    "FIGSIZE_2x3 = (18, 10)\n",
    "FIGSIZE_2x2 = (18, 18)\n",
    "TITLE_SIZE = 20\n",
    "LABEL_SIZE = 20\n",
    "SUPTITLE_SIZE = 24\n",
    "ANNOTATION_PROPS = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# Reference values for spectral statistics\n",
    "GOE_R = 0.5295\n",
    "POISSON_R = 0.386\n",
    "GOE_Z = 0.5687\n",
    "POISSON_Z = 0.5\n",
    "\n",
    "print(\"Imports and constants loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Utility Functions\n",
    "\n",
    "def parse_1dSSHkappa_filename(filepath):\n",
    "    \"\"\"\n",
    "    Parse a 1dSSHkappa data filename to extract parameters.\n",
    "    \n",
    "    Expected format: 1dSSHkappa_L{L}_w_{w}_{kappa}_disorder{start}-{end}_numEigs{n}_realizations{r}_{YYYY-MM-DD-HHMMSS}_*.dat\n",
    "    \n",
    "    Returns dict with keys: L, w, kappa, disorder_start, disorder_end, num_eigs, num_realizations, date, timestamp\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filepath)\n",
    "    params = {}\n",
    "    \n",
    "    # Extract L\n",
    "    match = re.search(r'_L(\\d+)_', basename)\n",
    "    if match:\n",
    "        params['L'] = int(match.group(1))\n",
    "    \n",
    "    # Extract w (hopping parameter)\n",
    "    match = re.search(r'_w_([\\d.]+)_', basename)\n",
    "    if match:\n",
    "        params['w'] = float(match.group(1))\n",
    "    \n",
    "    # Extract kappa (spectral localizer coupling)\n",
    "    # Pattern: _w_{w}_{kappa}_disorder - kappa is after w and before disorder\n",
    "    match = re.search(r'_w_[\\d.]+_([\\d.]+)_disorder', basename)\n",
    "    if match:\n",
    "        params['kappa'] = float(match.group(1))\n",
    "    \n",
    "    # Extract disorder range\n",
    "    match = re.search(r'_disorder([\\d.]+)-([\\d.]+)_', basename)\n",
    "    if match:\n",
    "        params['disorder_start'] = float(match.group(1))\n",
    "        params['disorder_end'] = float(match.group(2))\n",
    "    \n",
    "    # Extract number of eigenvalues\n",
    "    match = re.search(r'_numEigs(\\d+)_', basename)\n",
    "    if match:\n",
    "        params['num_eigs'] = int(match.group(1))\n",
    "    \n",
    "    # Extract number of realizations\n",
    "    match = re.search(r'_realizations(\\d+)_', basename)\n",
    "    if match:\n",
    "        params['num_realizations'] = int(match.group(1))\n",
    "    \n",
    "    # Extract date and timestamp (format: YYYY-MM-DD-HHMMSS)\n",
    "    match = re.search(r'_(\\d{4}-\\d{2}-\\d{2})-(\\d{6})_', basename)\n",
    "    if match:\n",
    "        params['date'] = match.group(1)\n",
    "        params['timestamp'] = match.group(2)\n",
    "        params['datetime_str'] = f\"{match.group(1)}-{match.group(2)}\"\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def find_1dSSHkappa_files(data_dir):\n",
    "    \"\"\"\n",
    "    Find all 1dSSHkappa files and organize by parameters.\n",
    "    \n",
    "    For each kappa, keeps only the latest timestamp.\n",
    "    Verifies all 6 required file types exist.\n",
    "    \n",
    "    Returns: Nested dict: {(L, w, d_start, d_end): {kappa: {'files': {...}, 'params': {...}}}}\n",
    "    \"\"\"\n",
    "    # Required file suffixes\n",
    "    required_suffixes = [\n",
    "        '_H_eigval.dat',\n",
    "        '_H_IPR.dat',\n",
    "        '_spectral_localiser_eigval.dat',\n",
    "        '_spectral_localiser_IPR.dat',\n",
    "        '_seeds.dat',\n",
    "        '_parameters.txt'\n",
    "    ]\n",
    "    \n",
    "    # Find all parameter files first (they define complete file sets)\n",
    "    param_files = glob.glob(os.path.join(data_dir, '1dSSHkappa_L*_parameters.txt'))\n",
    "    \n",
    "    # Group files by parameter set\n",
    "    file_groups = defaultdict(lambda: defaultdict(dict))\n",
    "    \n",
    "    for param_file in param_files:\n",
    "        params = parse_1dSSHkappa_filename(param_file)\n",
    "        if not params:\n",
    "            continue\n",
    "            \n",
    "        # Extract base path (everything before the suffix)\n",
    "        base_path = param_file.replace('_parameters.txt', '')\n",
    "        \n",
    "        # Check all required files exist\n",
    "        files_dict = {}\n",
    "        all_exist = True\n",
    "        for suffix in required_suffixes:\n",
    "            file_path = base_path + suffix\n",
    "            if os.path.exists(file_path):\n",
    "                # Create a key from the suffix\n",
    "                key = suffix.replace('.dat', '').replace('.txt', '').strip('_')\n",
    "                files_dict[key] = file_path\n",
    "            else:\n",
    "                all_exist = False\n",
    "                break\n",
    "        \n",
    "        if not all_exist:\n",
    "            continue\n",
    "        \n",
    "        # Create parameter key\n",
    "        L = params.get('L', 0)\n",
    "        w = params.get('w', 0)\n",
    "        d_start = params.get('disorder_start', 0)\n",
    "        d_end = params.get('disorder_end', 0)\n",
    "        kappa = params.get('kappa', 0)\n",
    "        datetime_str = params.get('datetime_str', '')\n",
    "        \n",
    "        param_key = (L, w, d_start, d_end)\n",
    "        \n",
    "        # Keep only the latest timestamp for each kappa\n",
    "        if kappa in file_groups[param_key]:\n",
    "            existing_datetime = file_groups[param_key][kappa].get('datetime_str', '')\n",
    "            if datetime_str > existing_datetime:\n",
    "                file_groups[param_key][kappa] = {\n",
    "                    'files': files_dict,\n",
    "                    'params': params,\n",
    "                    'datetime_str': datetime_str\n",
    "                }\n",
    "        else:\n",
    "            file_groups[param_key][kappa] = {\n",
    "                'files': files_dict,\n",
    "                'params': params,\n",
    "                'datetime_str': datetime_str\n",
    "            }\n",
    "    \n",
    "    return dict(file_groups)\n",
    "\n",
    "\n",
    "def load_parameters_file(parameters_txt_path):\n",
    "    \"\"\"\n",
    "    Parse a parameters.txt file.\n",
    "    \n",
    "    Returns dict with disorder_resolution, L, w, kappa, rho, etc.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    with open(parameters_txt_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if '=' in line:\n",
    "                key, value = line.split('=', 1)\n",
    "                key = key.strip()\n",
    "                value = value.strip()\n",
    "                # Try to convert to appropriate type\n",
    "                try:\n",
    "                    if '.' in value:\n",
    "                        params[key] = float(value)\n",
    "                    else:\n",
    "                        params[key] = int(value)\n",
    "                except ValueError:\n",
    "                    params[key] = value\n",
    "    return params\n",
    "\n",
    "\n",
    "def load_1dSSHkappa_data(file_dict, params_from_filename):\n",
    "    \"\"\"\n",
    "    Load all data files using memmap for memory efficiency.\n",
    "    \n",
    "    Returns dict with loaded data + metadata.\n",
    "    \"\"\"\n",
    "    files = file_dict['files']\n",
    "    \n",
    "    # Load parameters file for shape info\n",
    "    params = load_parameters_file(files['parameters'])\n",
    "    \n",
    "    # Determine array shape\n",
    "    disorder_resolution = int(params.get('disorder_resolution', 21))\n",
    "    num_realizations = int(params.get('num_realizations', params_from_filename.get('num_realizations', 500)))\n",
    "    L = int(params.get('L', params_from_filename.get('L', 3000)))\n",
    "    \n",
    "    shape = (disorder_resolution, num_realizations, L)\n",
    "    \n",
    "    # Create disorder values array\n",
    "    disorder_start = float(params.get('disorder_start', params_from_filename.get('disorder_start', 0.0)))\n",
    "    disorder_end = float(params.get('disorder_end', params_from_filename.get('disorder_end', 1.0)))\n",
    "    disorder_values = np.linspace(disorder_start, disorder_end, disorder_resolution)\n",
    "    \n",
    "    # Load data using memmap\n",
    "    data = {\n",
    "        'params': params,\n",
    "        'disorder_values': disorder_values,\n",
    "        'shape': shape,\n",
    "        'L': L,\n",
    "        'disorder_resolution': disorder_resolution,\n",
    "        'num_realizations': num_realizations,\n",
    "        'kappa': params_from_filename.get('kappa', params.get('kappa', 0))\n",
    "    }\n",
    "    \n",
    "    # Load eigenvalues and IPR using memmap\n",
    "    if 'H_eigval' in files:\n",
    "        data['H_eigval'] = np.memmap(files['H_eigval'], dtype='float64', mode='r', shape=shape)\n",
    "    \n",
    "    if 'H_IPR' in files:\n",
    "        data['H_IPR'] = np.memmap(files['H_IPR'], dtype='float64', mode='r', shape=shape)\n",
    "    \n",
    "    if 'spectral_localiser_eigval' in files:\n",
    "        data['SL_eigval'] = np.memmap(files['spectral_localiser_eigval'], dtype='float64', mode='r', shape=shape)\n",
    "    \n",
    "    if 'spectral_localiser_IPR' in files:\n",
    "        data['SL_IPR'] = np.memmap(files['spectral_localiser_IPR'], dtype='float64', mode='r', shape=shape)\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"Utility functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Statistics Functions\n",
    "\n",
    "def calculate_r(eigval):\n",
    "    \"\"\"\n",
    "    Calculate the adjacent gap ratio r for a sorted array of eigenvalues.\n",
    "    r = min(s_i, s_{i+1}) / max(s_i, s_{i+1})\n",
    "    \n",
    "    Returns mean r value.\n",
    "    \"\"\"\n",
    "    eigval_sorted = np.sort(eigval)\n",
    "    spacings = np.diff(eigval_sorted)\n",
    "    \n",
    "    min_vals = np.minimum(spacings[:-1], spacings[1:])\n",
    "    max_vals = np.maximum(spacings[:-1], spacings[1:])\n",
    "    \n",
    "    r = np.divide(min_vals, max_vals, out=np.zeros_like(min_vals), where=max_vals != 0)\n",
    "    return r.mean()\n",
    "\n",
    "\n",
    "def calculate_z(eigval):\n",
    "    \"\"\"\n",
    "    Calculate the next-nearest neighbor ratio z.\n",
    "    \n",
    "    Returns mean z value.\n",
    "    \"\"\"\n",
    "    eigval_sorted = np.sort(eigval)\n",
    "    s = np.diff(eigval_sorted)\n",
    "    \n",
    "    if len(s) < 5:\n",
    "        return np.nan\n",
    "    \n",
    "    s_i_minus_2 = s[:-4]\n",
    "    s_i_minus_1 = s[1:-3]\n",
    "    s_i = s[2:-2]\n",
    "    s_i_plus_1 = s[3:-1]\n",
    "    \n",
    "    nn = np.minimum(s_i, s_i_minus_1)\n",
    "    n_other = np.maximum(s_i, s_i_minus_1)\n",
    "    nnn_left = s_i_minus_1 + s_i_minus_2\n",
    "    nnn_right = s_i + s_i_plus_1\n",
    "    \n",
    "    nnn = np.minimum.reduce([n_other, nnn_left, nnn_right])\n",
    "    \n",
    "    z = np.divide(nn, nnn, out=np.zeros_like(nn), where=nnn != 0)\n",
    "    return z.mean()\n",
    "\n",
    "\n",
    "def compute_rz_statistics(eigval_array):\n",
    "    \"\"\"\n",
    "    Compute r and z statistics for each realization in an array.\n",
    "    \n",
    "    Args:\n",
    "        eigval_array: 2D array of shape (num_realizations, num_eigenvalues)\n",
    "        \n",
    "    Returns:\n",
    "        r_values: array of r values for each realization\n",
    "        z_values: array of z values for each realization\n",
    "    \"\"\"\n",
    "    num_realizations = eigval_array.shape[0]\n",
    "    r_values = np.zeros(num_realizations)\n",
    "    z_values = np.zeros(num_realizations)\n",
    "    \n",
    "    for i in range(num_realizations):\n",
    "        r_values[i] = calculate_r(eigval_array[i, :])\n",
    "        z_values[i] = calculate_z(eigval_array[i, :])\n",
    "    \n",
    "    return r_values, z_values\n",
    "\n",
    "\n",
    "print(\"Statistics functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Plotting Functions\n",
    "\n",
    "def plot_3x2_grid(H_eigval, H_IPR, SL_eigval, SL_IPR, title_suffix, save_path=None):\n",
    "    \"\"\"\n",
    "    Create a 3x2 grid comparing Hamiltonian and Spectral Localizer.\n",
    "    \n",
    "    Layout:\n",
    "        [H DOS]    [H Index Plot]    [H IPR]\n",
    "        [SL DOS]   [SL Index Plot]   [SL IPR]\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 3, figsize=FIGSIZE_2x3, constrained_layout=True)\n",
    "    \n",
    "    # Column 0: DOS (horizontal histograms)\n",
    "    axs[0, 0].hist(H_eigval, bins=100, density=True, orientation='horizontal',\n",
    "                   color=COLORS['H'], alpha=0.8)\n",
    "    axs[0, 0].set_title('Hamiltonian DOS', size=TITLE_SIZE)\n",
    "    axs[0, 0].set_xlabel('P(E)', size=LABEL_SIZE)\n",
    "    axs[0, 0].set_ylabel('Energy (E)', size=LABEL_SIZE)\n",
    "    axs[0, 0].grid(True)\n",
    "    axs[0, 0].set_axisbelow(True)\n",
    "    \n",
    "    axs[1, 0].hist(SL_eigval, bins=100, density=True, orientation='horizontal',\n",
    "                   color=COLORS['SL'], alpha=0.8)\n",
    "    axs[1, 0].set_title('Spectral Localiser DOS', size=TITLE_SIZE)\n",
    "    axs[1, 0].set_xlabel('P(E)', size=LABEL_SIZE)\n",
    "    axs[1, 0].set_ylabel('Eigenvalue', size=LABEL_SIZE)\n",
    "    axs[1, 0].grid(True)\n",
    "    axs[1, 0].set_axisbelow(True)\n",
    "    \n",
    "    # Column 1: Eigenvalues vs Index\n",
    "    H_indices = np.arange(len(H_eigval))\n",
    "    SL_indices = np.arange(len(SL_eigval))\n",
    "    \n",
    "    axs[0, 1].scatter(H_indices, np.sort(H_eigval), s=1, c=COLORS['H'], alpha=0.5)\n",
    "    axs[0, 1].set_title('Hamiltonian Eigenvalues', size=TITLE_SIZE)\n",
    "    axs[0, 1].set_xlabel('Index', size=LABEL_SIZE)\n",
    "    axs[0, 1].set_ylabel('Energy (E)', size=LABEL_SIZE)\n",
    "    axs[0, 1].grid(True)\n",
    "    axs[0, 1].set_axisbelow(True)\n",
    "    \n",
    "    axs[1, 1].scatter(SL_indices, np.sort(SL_eigval), s=1, c=COLORS['SL'], alpha=0.5)\n",
    "    axs[1, 1].set_title('Spectral Localiser Eigenvalues', size=TITLE_SIZE)\n",
    "    axs[1, 1].set_xlabel('Index', size=LABEL_SIZE)\n",
    "    axs[1, 1].set_ylabel('Eigenvalue', size=LABEL_SIZE)\n",
    "    axs[1, 1].grid(True)\n",
    "    axs[1, 1].set_axisbelow(True)\n",
    "    \n",
    "    # Column 2: IPR vs Index (sorted by eigenvalue)\n",
    "    H_sort_idx = np.argsort(H_eigval)\n",
    "    SL_sort_idx = np.argsort(SL_eigval)\n",
    "    \n",
    "    axs[0, 2].scatter(H_indices, H_IPR[H_sort_idx], s=1, c=COLORS['H'], alpha=0.5)\n",
    "    axs[0, 2].set_title('Hamiltonian IPR', size=TITLE_SIZE)\n",
    "    axs[0, 2].set_xlabel('Index', size=LABEL_SIZE)\n",
    "    axs[0, 2].set_ylabel('IPR', size=LABEL_SIZE)\n",
    "    axs[0, 2].grid(True)\n",
    "    axs[0, 2].set_axisbelow(True)\n",
    "    \n",
    "    # Add average IPR annotation\n",
    "    avg_H_IPR = np.mean(H_IPR)\n",
    "    axs[0, 2].text(0.95, 0.95, f'avg={avg_H_IPR:.4f}', transform=axs[0, 2].transAxes,\n",
    "                   fontsize=14, verticalalignment='top', horizontalalignment='right',\n",
    "                   bbox=ANNOTATION_PROPS)\n",
    "    \n",
    "    axs[1, 2].scatter(SL_indices, SL_IPR[SL_sort_idx], s=1, c=COLORS['SL'], alpha=0.5)\n",
    "    axs[1, 2].set_title('Spectral Localiser IPR', size=TITLE_SIZE)\n",
    "    axs[1, 2].set_xlabel('Index', size=LABEL_SIZE)\n",
    "    axs[1, 2].set_ylabel('IPR', size=LABEL_SIZE)\n",
    "    axs[1, 2].grid(True)\n",
    "    axs[1, 2].set_axisbelow(True)\n",
    "    \n",
    "    avg_SL_IPR = np.mean(SL_IPR)\n",
    "    axs[1, 2].text(0.95, 0.95, f'avg={avg_SL_IPR:.4f}', transform=axs[1, 2].transAxes,\n",
    "                   fontsize=14, verticalalignment='top', horizontalalignment='right',\n",
    "                   bbox=ANNOTATION_PROPS)\n",
    "    \n",
    "    fig.suptitle(f'DOS, Eigenvalues and IPR: {title_suffix}', fontsize=SUPTITLE_SIZE)\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "def plot_rz_vs_disorder_for_kappas(data_by_kappa, save_path=None):\n",
    "    \"\"\"\n",
    "    Create 2x2 grid with disorder on x-axis and multiple kappa values as lines.\n",
    "    \n",
    "    Layout:\n",
    "        [H r vs disorder]    [H z vs disorder]\n",
    "        [SL r vs disorder]   [SL z vs disorder]\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=FIGSIZE_2x2, constrained_layout=True)\n",
    "    \n",
    "    # Sort kappas for consistent legend ordering\n",
    "    sorted_kappas = sorted(data_by_kappa.keys())\n",
    "    \n",
    "    # Color map for different kappas\n",
    "    cmap = plt.cm.viridis\n",
    "    colors = [cmap(i / len(sorted_kappas)) for i in range(len(sorted_kappas))]\n",
    "    \n",
    "    for kappa_idx, kappa in enumerate(sorted_kappas):\n",
    "        data = data_by_kappa[kappa]\n",
    "        disorder_values = data['disorder_values']\n",
    "        num_disorder = len(disorder_values)\n",
    "        num_realizations = data['num_realizations']\n",
    "        sqrt_n = np.sqrt(num_realizations)\n",
    "        \n",
    "        # Compute statistics for each disorder level\n",
    "        H_r_means, H_r_stds = [], []\n",
    "        H_z_means, H_z_stds = [], []\n",
    "        SL_r_means, SL_r_stds = [], []\n",
    "        SL_z_means, SL_z_stds = [], []\n",
    "        \n",
    "        for d_idx in range(num_disorder):\n",
    "            # Compute r and z for each realization\n",
    "            H_r_vals, H_z_vals = compute_rz_statistics(data['H_eigval'][d_idx, :, :])\n",
    "            SL_r_vals, SL_z_vals = compute_rz_statistics(data['SL_eigval'][d_idx, :, :])\n",
    "            \n",
    "            H_r_means.append(np.mean(H_r_vals))\n",
    "            H_r_stds.append(np.std(H_r_vals) / sqrt_n)\n",
    "            H_z_means.append(np.mean(H_z_vals))\n",
    "            H_z_stds.append(np.std(H_z_vals) / sqrt_n)\n",
    "            \n",
    "            SL_r_means.append(np.mean(SL_r_vals))\n",
    "            SL_r_stds.append(np.std(SL_r_vals) / sqrt_n)\n",
    "            SL_z_means.append(np.mean(SL_z_vals))\n",
    "            SL_z_stds.append(np.std(SL_z_vals) / sqrt_n)\n",
    "        \n",
    "        color = colors[kappa_idx]\n",
    "        label = f'κ={kappa}'\n",
    "        \n",
    "        # Plot H statistics\n",
    "        axs[0, 0].errorbar(disorder_values, H_r_means, yerr=H_r_stds,\n",
    "                           label=label, marker='o', capsize=3, linestyle='-', color=color)\n",
    "        axs[0, 1].errorbar(disorder_values, H_z_means, yerr=H_z_stds,\n",
    "                           label=label, marker='o', capsize=3, linestyle='-', color=color)\n",
    "        \n",
    "        # Plot SL statistics\n",
    "        axs[1, 0].errorbar(disorder_values, SL_r_means, yerr=SL_r_stds,\n",
    "                           label=label, marker='o', capsize=3, linestyle='-', color=color)\n",
    "        axs[1, 1].errorbar(disorder_values, SL_z_means, yerr=SL_z_stds,\n",
    "                           label=label, marker='o', capsize=3, linestyle='-', color=color)\n",
    "    \n",
    "    # Add reference lines for r plots (left column)\n",
    "    for ax in [axs[0, 0], axs[1, 0]]:\n",
    "        ax.axhline(y=GOE_R, color='red', linestyle='--', label='GOE', alpha=0.7)\n",
    "        ax.axhline(y=POISSON_R, color='green', linestyle='--', label='Poisson', alpha=0.7)\n",
    "        ax.set_xlabel('Disorder Strength', size=LABEL_SIZE)\n",
    "        ax.set_ylabel('r', size=LABEL_SIZE)\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Add reference lines for z plots (right column)\n",
    "    for ax in [axs[0, 1], axs[1, 1]]:\n",
    "        ax.axhline(y=GOE_Z, color='red', linestyle='--', label='GOE', alpha=0.7)\n",
    "        ax.axhline(y=POISSON_Z, color='green', linestyle='--', label='Poisson', alpha=0.7)\n",
    "        ax.set_xlabel('Disorder Strength', size=LABEL_SIZE)\n",
    "        ax.set_ylabel('z', size=LABEL_SIZE)\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Set titles\n",
    "    axs[0, 0].set_title('r of Hamiltonian', size=TITLE_SIZE)\n",
    "    axs[0, 1].set_title('z of Hamiltonian', size=TITLE_SIZE)\n",
    "    axs[1, 0].set_title('r of Spectral Localizer', size=TITLE_SIZE)\n",
    "    axs[1, 1].set_title('z of Spectral Localizer', size=TITLE_SIZE)\n",
    "    \n",
    "    fig.suptitle('Spectral Statistics vs Disorder (multiple κ values)', fontsize=SUPTITLE_SIZE)\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "def plot_rz_vs_kappa_for_disorders(data_by_kappa, selected_disorder_indices, save_path=None):\n",
    "    \"\"\"\n",
    "    Create 2x2 grid with kappa on x-axis and multiple disorder levels as lines.\n",
    "    \n",
    "    Layout:\n",
    "        [H r vs kappa]    [H z vs kappa]\n",
    "        [SL r vs kappa]   [SL z vs kappa]\n",
    "    \"\"\"\n",
    "    fig, axs = plt.subplots(2, 2, figsize=FIGSIZE_2x2, constrained_layout=True)\n",
    "    \n",
    "    # Get kappas and disorder values\n",
    "    sorted_kappas = sorted(data_by_kappa.keys())\n",
    "    first_data = data_by_kappa[sorted_kappas[0]]\n",
    "    disorder_values = first_data['disorder_values']\n",
    "    num_realizations = first_data['num_realizations']\n",
    "    sqrt_n = np.sqrt(num_realizations)\n",
    "    \n",
    "    # Color map for different disorder levels\n",
    "    cmap = plt.cm.plasma\n",
    "    colors = [cmap(i / len(selected_disorder_indices)) for i in range(len(selected_disorder_indices))]\n",
    "    \n",
    "    for d_color_idx, d_idx in enumerate(selected_disorder_indices):\n",
    "        W = disorder_values[d_idx]\n",
    "        \n",
    "        H_r_means, H_r_stds = [], []\n",
    "        H_z_means, H_z_stds = [], []\n",
    "        SL_r_means, SL_r_stds = [], []\n",
    "        SL_z_means, SL_z_stds = [], []\n",
    "        \n",
    "        for kappa in sorted_kappas:\n",
    "            data = data_by_kappa[kappa]\n",
    "            \n",
    "            # Compute r and z for this disorder level across all realizations\n",
    "            H_r_vals, H_z_vals = compute_rz_statistics(data['H_eigval'][d_idx, :, :])\n",
    "            SL_r_vals, SL_z_vals = compute_rz_statistics(data['SL_eigval'][d_idx, :, :])\n",
    "            \n",
    "            H_r_means.append(np.mean(H_r_vals))\n",
    "            H_r_stds.append(np.std(H_r_vals) / sqrt_n)\n",
    "            H_z_means.append(np.mean(H_z_vals))\n",
    "            H_z_stds.append(np.std(H_z_vals) / sqrt_n)\n",
    "            \n",
    "            SL_r_means.append(np.mean(SL_r_vals))\n",
    "            SL_r_stds.append(np.std(SL_r_vals) / sqrt_n)\n",
    "            SL_z_means.append(np.mean(SL_z_vals))\n",
    "            SL_z_stds.append(np.std(SL_z_vals) / sqrt_n)\n",
    "        \n",
    "        color = colors[d_color_idx]\n",
    "        label = f'W={W:.2f}'\n",
    "        \n",
    "        # Plot H statistics\n",
    "        axs[0, 0].errorbar(sorted_kappas, H_r_means, yerr=H_r_stds,\n",
    "                           label=label, marker='o', capsize=3, linestyle='-', color=color)\n",
    "        axs[0, 1].errorbar(sorted_kappas, H_z_means, yerr=H_z_stds,\n",
    "                           label=label, marker='o', capsize=3, linestyle='-', color=color)\n",
    "        \n",
    "        # Plot SL statistics\n",
    "        axs[1, 0].errorbar(sorted_kappas, SL_r_means, yerr=SL_r_stds,\n",
    "                           label=label, marker='o', capsize=3, linestyle='-', color=color)\n",
    "        axs[1, 1].errorbar(sorted_kappas, SL_z_means, yerr=SL_z_stds,\n",
    "                           label=label, marker='o', capsize=3, linestyle='-', color=color)\n",
    "    \n",
    "    # Add reference lines for r plots (left column)\n",
    "    for ax in [axs[0, 0], axs[1, 0]]:\n",
    "        ax.axhline(y=GOE_R, color='red', linestyle='--', label='GOE', alpha=0.7)\n",
    "        ax.axhline(y=POISSON_R, color='green', linestyle='--', label='Poisson', alpha=0.7)\n",
    "        ax.set_xlabel('κ (Spectral Localizer Coupling)', size=LABEL_SIZE)\n",
    "        ax.set_ylabel('r', size=LABEL_SIZE)\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Add reference lines for z plots (right column)\n",
    "    for ax in [axs[0, 1], axs[1, 1]]:\n",
    "        ax.axhline(y=GOE_Z, color='red', linestyle='--', label='GOE', alpha=0.7)\n",
    "        ax.axhline(y=POISSON_Z, color='green', linestyle='--', label='Poisson', alpha=0.7)\n",
    "        ax.set_xlabel('κ (Spectral Localizer Coupling)', size=LABEL_SIZE)\n",
    "        ax.set_ylabel('z', size=LABEL_SIZE)\n",
    "        ax.legend(loc='best', fontsize=10)\n",
    "        ax.grid(True)\n",
    "    \n",
    "    # Set titles\n",
    "    axs[0, 0].set_title('r of Hamiltonian', size=TITLE_SIZE)\n",
    "    axs[0, 1].set_title('z of Hamiltonian', size=TITLE_SIZE)\n",
    "    axs[1, 0].set_title('r of Spectral Localizer', size=TITLE_SIZE)\n",
    "    axs[1, 1].set_title('z of Spectral Localizer', size=TITLE_SIZE)\n",
    "    \n",
    "    fig.suptitle('Spectral Statistics vs κ (multiple disorder levels)', fontsize=SUPTITLE_SIZE)\n",
    "    \n",
    "    if save_path:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {save_path}\")\n",
    "    \n",
    "    return fig, axs\n",
    "\n",
    "\n",
    "print(\"Plotting functions defined successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Configuration (User-Editable)\n",
    "\n",
    "# =============================================================================\n",
    "# DATA DIRECTORY\n",
    "# =============================================================================\n",
    "DATA_DIR = '../data/'\n",
    "\n",
    "# =============================================================================\n",
    "# FILE SELECTION\n",
    "# =============================================================================\n",
    "# Available kappa values: 0.5, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 2.0\n",
    "selected_kappas = [0.5, 0.8, 0.85, 0.9, 0.95, 1.0, 1.05, 1.1, 1.15, 1.2, 2.0]  # Or a subset\n",
    "\n",
    "# Target system parameters\n",
    "target_L = 3000\n",
    "target_w = 1.0\n",
    "target_disorder_start = 0.0\n",
    "target_disorder_end = 1.0\n",
    "\n",
    "# =============================================================================\n",
    "# PLOT OPTIONS\n",
    "# =============================================================================\n",
    "# For 3x2 grids: which disorder indices to plot?\n",
    "# None = auto-select [0, mid, -1] (low, medium, high disorder)\n",
    "# Or specify: [0, 5, 10, 20] for specific indices\n",
    "disorder_indices_to_plot = None\n",
    "\n",
    "# For r/z vs kappa: which disorder level indices to include?\n",
    "# None = all disorder levels\n",
    "disorder_indices_for_rz_vs_kappa = [0, 10, 20]  # Example: low, mid, high\n",
    "\n",
    "# =============================================================================\n",
    "# SAVE OPTIONS\n",
    "# =============================================================================\n",
    "SAVE_FIGURES = True\n",
    "FIGURE_DIR = '../figures/'\n",
    "\n",
    "print(\"Configuration loaded.\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Selected kappas: {selected_kappas}\")\n",
    "print(f\"  Target L={target_L}, w={target_w}\")\n",
    "print(f\"  Save figures: {SAVE_FIGURES}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Analysis 1 - 3x2 Grid Visualization\n",
    "\n",
    "print(\"Loading 1dSSHkappa files...\")\n",
    "all_files = find_1dSSHkappa_files(DATA_DIR)\n",
    "\n",
    "if not all_files:\n",
    "    print(\"WARNING: No 1dSSHkappa files found in the data directory!\")\n",
    "    print(f\"Searched in: {DATA_DIR}\")\n",
    "else:\n",
    "    print(f\"Found {len(all_files)} parameter set(s).\")\n",
    "    for param_key, kappa_dict in all_files.items():\n",
    "        L, w, d_start, d_end = param_key\n",
    "        print(f\"  L={L}, w={w}, disorder={d_start}-{d_end}: {len(kappa_dict)} kappa values\")\n",
    "\n",
    "# Filter by target parameters and create 3x2 grids\n",
    "for param_key, kappa_dict in all_files.items():\n",
    "    L, w, d_start, d_end = param_key\n",
    "    \n",
    "    # Check if this matches our target parameters\n",
    "    if L != target_L or w != target_w:\n",
    "        continue\n",
    "    if d_start != target_disorder_start or d_end != target_disorder_end:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing parameter set: L={L}, w={w}, disorder={d_start}-{d_end}\")\n",
    "    \n",
    "    for kappa, file_info in sorted(kappa_dict.items()):\n",
    "        if kappa not in selected_kappas:\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n  Loading kappa={kappa}...\")\n",
    "        data = load_1dSSHkappa_data(file_info, file_info['params'])\n",
    "        \n",
    "        # Determine disorder indices to plot\n",
    "        num_disorder = data['disorder_resolution']\n",
    "        if disorder_indices_to_plot is None:\n",
    "            d_indices = [0, num_disorder // 2, num_disorder - 1]\n",
    "        else:\n",
    "            d_indices = disorder_indices_to_plot\n",
    "        \n",
    "        # Create 3x2 grid for each selected disorder level\n",
    "        for d_idx in d_indices:\n",
    "            if d_idx >= num_disorder:\n",
    "                print(f\"    Skipping disorder index {d_idx} (out of range)\")\n",
    "                continue\n",
    "            \n",
    "            W = data['disorder_values'][d_idx]\n",
    "            print(f\"    Creating 3x2 grid for W={W:.2f}...\")\n",
    "            \n",
    "            # Flatten data across realizations\n",
    "            H_eigval_flat = data['H_eigval'][d_idx, :, :].flatten()\n",
    "            H_IPR_flat = data['H_IPR'][d_idx, :, :].flatten()\n",
    "            SL_eigval_flat = data['SL_eigval'][d_idx, :, :].flatten()\n",
    "            SL_IPR_flat = data['SL_IPR'][d_idx, :, :].flatten()\n",
    "            \n",
    "            # Plot\n",
    "            title = f'κ={kappa}, W={W:.2f}, L={L}'\n",
    "            save_name = os.path.join(FIGURE_DIR, f'1dSSHkappa_3x2_k{kappa}_W{W:.2f}.png') if SAVE_FIGURES else None\n",
    "            \n",
    "            fig, axs = plot_3x2_grid(H_eigval_flat, H_IPR_flat, SL_eigval_flat, SL_IPR_flat, title, save_name)\n",
    "            plt.show()\n",
    "\n",
    "print(\"\\n3x2 grid visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Analysis 2 - r/z Statistics\n",
    "\n",
    "print(\"Loading data for r/z statistics analysis...\")\n",
    "\n",
    "# =============================================================================\n",
    "# Part A: Load and Organize Data\n",
    "# =============================================================================\n",
    "\n",
    "grouped_data = {}\n",
    "for param_key, kappa_dict in all_files.items():\n",
    "    L, w, d_start, d_end = param_key\n",
    "    \n",
    "    # Check target parameters\n",
    "    if L != target_L or w != target_w:\n",
    "        continue\n",
    "    if d_start != target_disorder_start or d_end != target_disorder_end:\n",
    "        continue\n",
    "    \n",
    "    kappa_group = {}\n",
    "    for kappa, file_info in sorted(kappa_dict.items()):\n",
    "        if kappa not in selected_kappas:\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Loading kappa={kappa}...\")\n",
    "        data = load_1dSSHkappa_data(file_info, file_info['params'])\n",
    "        kappa_group[kappa] = data\n",
    "    \n",
    "    if kappa_group:\n",
    "        grouped_data[param_key] = kappa_group\n",
    "\n",
    "if not grouped_data:\n",
    "    print(\"WARNING: No data matched the target parameters!\")\n",
    "else:\n",
    "    print(f\"\\nLoaded {sum(len(kg) for kg in grouped_data.values())} kappa datasets.\")\n",
    "\n",
    "# =============================================================================\n",
    "# Part B: Plot r/z vs Disorder\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nGenerating r/z vs Disorder plots...\")\n",
    "for param_key, kappa_dict in grouped_data.items():\n",
    "    L, w, d_start, d_end = param_key\n",
    "    print(f\"  Parameter set: L={L}, w={w}, disorder={d_start}-{d_end}\")\n",
    "    \n",
    "    save_path = os.path.join(FIGURE_DIR, f'1dSSHkappa_rz_vs_disorder_L{L}.png') if SAVE_FIGURES else None\n",
    "    fig, axs = plot_rz_vs_disorder_for_kappas(kappa_dict, save_path=save_path)\n",
    "    plt.show()\n",
    "\n",
    "# =============================================================================\n",
    "# Part C: Plot r/z vs Kappa\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nGenerating r/z vs Kappa plots...\")\n",
    "for param_key, kappa_dict in grouped_data.items():\n",
    "    L, w, d_start, d_end = param_key\n",
    "    print(f\"  Parameter set: L={L}, w={w}, disorder={d_start}-{d_end}\")\n",
    "    \n",
    "    # Determine disorder indices\n",
    "    first_data = kappa_dict[list(kappa_dict.keys())[0]]\n",
    "    num_disorder = first_data['disorder_resolution']\n",
    "    \n",
    "    if disorder_indices_for_rz_vs_kappa is None:\n",
    "        d_indices = list(range(num_disorder))\n",
    "    else:\n",
    "        d_indices = [idx for idx in disorder_indices_for_rz_vs_kappa if idx < num_disorder]\n",
    "    \n",
    "    print(f\"    Using disorder indices: {d_indices}\")\n",
    "    print(f\"    Corresponding W values: {[first_data['disorder_values'][i] for i in d_indices]}\")\n",
    "    \n",
    "    save_path = os.path.join(FIGURE_DIR, f'1dSSHkappa_rz_vs_kappa_L{L}.png') if SAVE_FIGURES else None\n",
    "    fig, axs = plot_rz_vs_kappa_for_disorders(kappa_dict, d_indices, save_path=save_path)\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nr/z statistics analysis complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
