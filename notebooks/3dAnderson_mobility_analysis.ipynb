{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Anderson Model Mobility Edge Analysis\n",
    "\n",
    "This notebook reproduces the plots from `analysis/3dAnderson-mobility-analysis.py` for interactive exploration and debugging.\n",
    "\n",
    "**Figures:**\n",
    "1. DOS and IPR Summary (2x3 grid)\n",
    "2. Energy-Resolved IPR (mobility edge visualization)\n",
    "3. Mobility Edge Trajectory vs disorder\n",
    "4. Filtered r/z Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import glob\n",
    "\n",
    "# Plot style constants\n",
    "COLORS = {'H': 'blue', 'SL': 'orange'}\n",
    "FIGSIZE_2x3 = (18, 10)\n",
    "FIGSIZE_2x2 = (18, 18)\n",
    "TITLE_SIZE = 20\n",
    "LABEL_SIZE = 20\n",
    "SUPTITLE_SIZE = 24\n",
    "ANNOTATION_PROPS = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "# Reference values for spectral statistics\n",
    "GOE_R = 0.5295\n",
    "POISSON_R = 0.386\n",
    "GOE_Z = 0.5687\n",
    "POISSON_Z = 0.5\n",
    "\n",
    "# IPR percentile cutoff for energy-resolved plots\n",
    "IPR_PERCENTILE_CUTOFF = 90\n",
    "\n",
    "# Data directory\n",
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filepath):\n",
    "    \"\"\"\n",
    "    Parse a 3D Anderson data filename to extract parameters.\n",
    "    Expected format: 3dAnderson_L{L}_disorder{start}-{end}_numEigs{n}_realizations{r}_{date}_*.dat\n",
    "    \"\"\"\n",
    "    basename = os.path.basename(filepath)\n",
    "    params = {}\n",
    "    \n",
    "    # Extract L\n",
    "    match = re.search(r'_L(\\d+)_', basename)\n",
    "    if match:\n",
    "        params['L'] = int(match.group(1))\n",
    "    \n",
    "    # Extract disorder range\n",
    "    match = re.search(r'_disorder([\\d.]+)-([\\d.]+)_', basename)\n",
    "    if match:\n",
    "        params['disorder_start'] = float(match.group(1))\n",
    "        params['disorder_end'] = float(match.group(2))\n",
    "    \n",
    "    # Extract number of realizations\n",
    "    match = re.search(r'_realizations(\\d+)_', basename)\n",
    "    if match:\n",
    "        params['num_realizations'] = int(match.group(1))\n",
    "    \n",
    "    return params\n",
    "\n",
    "\n",
    "def find_3d_anderson_files(data_dir, L=None):\n",
    "    \"\"\"\n",
    "    Find all 3D Anderson data files in the given directory.\n",
    "    Returns dict mapping base names to file paths by type.\n",
    "    \"\"\"\n",
    "    pattern = os.path.join(data_dir, '3dAnderson_L*_disorder*_*.dat')\n",
    "    all_files = glob.glob(pattern)\n",
    "    \n",
    "    # Group files by base name (everything before the file type suffix)\n",
    "    file_groups = {}\n",
    "    \n",
    "    for filepath in all_files:\n",
    "        basename = os.path.basename(filepath)\n",
    "        \n",
    "        # Determine the base name (strip the type suffix)\n",
    "        for suffix in ['_H_eigval.dat', '_H_eigvec.dat', '_H_IPR.dat',\n",
    "                       '_spectral_localiser_eigval.dat', '_spectral_localiser_eigvec.dat',\n",
    "                       '_spectral_localiser_IPR.dat', '_parameters.txt', '_seeds.dat']:\n",
    "            if basename.endswith(suffix):\n",
    "                base = basename[:-len(suffix)]\n",
    "                file_type = suffix[1:-4] if suffix.endswith('.dat') else suffix[1:-4]\n",
    "                \n",
    "                if base not in file_groups:\n",
    "                    file_groups[base] = {'params': parse_filename(filepath), 'files': {}}\n",
    "                \n",
    "                # Clean up file type name\n",
    "                if suffix == '_H_eigval.dat':\n",
    "                    file_groups[base]['files']['H_eigval'] = filepath\n",
    "                elif suffix == '_H_IPR.dat':\n",
    "                    file_groups[base]['files']['H_IPR'] = filepath\n",
    "                elif suffix == '_spectral_localiser_eigval.dat':\n",
    "                    file_groups[base]['files']['SL_eigval'] = filepath\n",
    "                elif suffix == '_spectral_localiser_IPR.dat':\n",
    "                    file_groups[base]['files']['SL_IPR'] = filepath\n",
    "                elif suffix == '_parameters.txt':\n",
    "                    file_groups[base]['files']['parameters'] = filepath\n",
    "                break\n",
    "    \n",
    "    # Filter by L if specified\n",
    "    if L is not None:\n",
    "        file_groups = {k: v for k, v in file_groups.items() if v['params'].get('L') == L}\n",
    "    \n",
    "    return file_groups\n",
    "\n",
    "\n",
    "def infer_shape_from_file(filepath, num_realizations, basis_size, dtype='float64'):\n",
    "    \"\"\"\n",
    "    Infer array shape from file size.\n",
    "    Assumes shape is (disorder_resolution, num_realizations, num_eigs).\n",
    "    \"\"\"\n",
    "    bytes_per_element = 8 if dtype == 'float64' else 16  # complex128\n",
    "    file_size = os.path.getsize(filepath)\n",
    "    total_elements = file_size // bytes_per_element\n",
    "    \n",
    "    # Try to infer disorder_resolution assuming num_eigs = basis_size\n",
    "    if total_elements % (num_realizations * basis_size) == 0:\n",
    "        disorder_resolution = total_elements // (num_realizations * basis_size)\n",
    "        num_eigs = basis_size\n",
    "    else:\n",
    "        # Try different disorder resolutions\n",
    "        for dr in range(1, 100):\n",
    "            if total_elements % (dr * num_realizations) == 0:\n",
    "                possible_num_eigs = total_elements // (dr * num_realizations)\n",
    "                if possible_num_eigs <= basis_size:\n",
    "                    disorder_resolution = dr\n",
    "                    num_eigs = possible_num_eigs\n",
    "                    break\n",
    "        else:\n",
    "            raise ValueError(f\"Cannot infer shape from file size {file_size}\")\n",
    "    \n",
    "    return (disorder_resolution, num_realizations, num_eigs)\n",
    "\n",
    "\n",
    "print(\"Data loading utilities defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data\n",
    "\n",
    "**IMPORTANT**: Set your parameters here. The shape is inferred from each file independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# USER CONFIGURATION - Modify these values to match your data\n",
    "# ============================================================================\n",
    "\n",
    "# System size\n",
    "L = 8\n",
    "\n",
    "# Number of disorder realizations\n",
    "num_realizations = 100\n",
    "\n",
    "# Disorder range\n",
    "disorder_start = 0.0\n",
    "disorder_end = 30.0\n",
    "\n",
    "# Basis sizes (for 3D Anderson)\n",
    "H_basis_size = L ** 3        # 512 for L=8\n",
    "SL_basis_size = 4 * L ** 3   # 2048 for L=8\n",
    "\n",
    "# ============================================================================\n",
    "# Find and load data files\n",
    "# ============================================================================\n",
    "\n",
    "file_groups = find_3d_anderson_files(DATA_DIR, L=L)\n",
    "print(f\"Found {len(file_groups)} data set(s) for L={L}:\")\n",
    "for name, info in file_groups.items():\n",
    "    print(f\"  {name}\")\n",
    "    print(f\"    Files: {list(info['files'].keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select which dataset to use (if multiple exist)\n",
    "# You can change this to select a different dataset\n",
    "dataset_name = list(file_groups.keys())[0] if file_groups else None\n",
    "\n",
    "if dataset_name is None:\n",
    "    raise ValueError(\"No data files found!\")\n",
    "\n",
    "files = file_groups[dataset_name]['files']\n",
    "print(f\"\\nUsing dataset: {dataset_name}\")\n",
    "print(f\"Available files: {list(files.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with shape inference from each file\n",
    "data = {}\n",
    "\n",
    "# Load H eigenvalues\n",
    "if 'H_eigval' in files:\n",
    "    shape = infer_shape_from_file(files['H_eigval'], num_realizations, H_basis_size)\n",
    "    data['H_eigval'] = np.memmap(files['H_eigval'], dtype='float64', mode='r', shape=shape)\n",
    "    print(f\"H_eigval: shape {shape}, range [{data['H_eigval'].min():.4f}, {data['H_eigval'].max():.4f}]\")\n",
    "    disorder_resolution = shape[0]\n",
    "\n",
    "# Load H IPR (infer shape independently!)\n",
    "if 'H_IPR' in files:\n",
    "    shape = infer_shape_from_file(files['H_IPR'], num_realizations, H_basis_size)\n",
    "    data['H_IPR'] = np.memmap(files['H_IPR'], dtype='float64', mode='r', shape=shape)\n",
    "    print(f\"H_IPR: shape {shape}, range [{data['H_IPR'].min():.6f}, {data['H_IPR'].max():.6f}]\")\n",
    "    print(f\"  Mean: {data['H_IPR'].mean():.6f}, Zeros: {np.sum(data['H_IPR'] == 0)}\")\n",
    "    disorder_resolution = shape[0]\n",
    "\n",
    "# Load SL eigenvalues\n",
    "if 'SL_eigval' in files:\n",
    "    shape = infer_shape_from_file(files['SL_eigval'], num_realizations, SL_basis_size)\n",
    "    data['SL_eigval'] = np.memmap(files['SL_eigval'], dtype='float64', mode='r', shape=shape)\n",
    "    print(f\"SL_eigval: shape {shape}, range [{data['SL_eigval'].min():.4f}, {data['SL_eigval'].max():.4f}]\")\n",
    "\n",
    "# Load SL IPR (infer shape independently!)\n",
    "if 'SL_IPR' in files:\n",
    "    shape = infer_shape_from_file(files['SL_IPR'], num_realizations, SL_basis_size)\n",
    "    data['SL_IPR'] = np.memmap(files['SL_IPR'], dtype='float64', mode='r', shape=shape)\n",
    "    print(f\"SL_IPR: shape {shape}, range [{data['SL_IPR'].min():.6f}, {data['SL_IPR'].max():.6f}]\")\n",
    "    print(f\"  Mean: {data['SL_IPR'].mean():.6f}, Zeros: {np.sum(data['SL_IPR'] == 0)}\")\n",
    "\n",
    "# Create disorder values array\n",
    "disorder_values = np.linspace(disorder_start, disorder_end, disorder_resolution)\n",
    "print(f\"\\nDisorder values: {disorder_resolution} points from {disorder_start} to {disorder_end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_ipr(eigvec):\n",
    "    \"\"\"Compute IPR from eigenvectors. IPR = sum(|psi_i|^4)\"\"\"\n",
    "    return np.sum(np.abs(eigvec) ** 4, axis=-1)\n",
    "\n",
    "\n",
    "def calculate_r(eigval):\n",
    "    \"\"\"Calculate the adjacent gap ratio r.\"\"\"\n",
    "    eigval_sorted = np.sort(eigval)\n",
    "    spacings = np.diff(eigval_sorted)\n",
    "    \n",
    "    min_vals = np.minimum(spacings[:-1], spacings[1:])\n",
    "    max_vals = np.maximum(spacings[:-1], spacings[1:])\n",
    "    \n",
    "    r = np.divide(min_vals, max_vals, out=np.zeros_like(min_vals), where=max_vals != 0)\n",
    "    return r.mean()\n",
    "\n",
    "\n",
    "def calculate_z(eigval):\n",
    "    \"\"\"Calculate the next-nearest neighbor ratio z.\"\"\"\n",
    "    eigval_sorted = np.sort(eigval)\n",
    "    s = np.diff(eigval_sorted)\n",
    "    \n",
    "    if len(s) < 5:\n",
    "        return np.nan\n",
    "    \n",
    "    s_i_minus_2 = s[:-4]\n",
    "    s_i_minus_1 = s[1:-3]\n",
    "    s_i = s[2:-2]\n",
    "    s_i_plus_1 = s[3:-1]\n",
    "    \n",
    "    nn = np.minimum(s_i, s_i_minus_1)\n",
    "    n_other = np.maximum(s_i, s_i_minus_1)\n",
    "    nnn_left = s_i_minus_1 + s_i_minus_2\n",
    "    nnn_right = s_i + s_i_plus_1\n",
    "    \n",
    "    nnn = np.minimum.reduce([n_other, nnn_left, nnn_right])\n",
    "    \n",
    "    z = np.divide(nn, nnn, out=np.zeros_like(nn), where=nnn != 0)\n",
    "    return z.mean()\n",
    "\n",
    "\n",
    "def extract_mobility_edge(eigval, ipr, num_bins=50, ipr_threshold=None):\n",
    "    \"\"\"\n",
    "    Extract the mobility edge from energy-resolved IPR data.\n",
    "    Returns E_c_lower, E_c_upper, bin_centers, bin_ipr\n",
    "    \"\"\"\n",
    "    if ipr_threshold is None:\n",
    "        ipr_threshold = 2.0 / len(eigval)\n",
    "    \n",
    "    E_min, E_max = eigval.min(), eigval.max()\n",
    "    bin_edges = np.linspace(E_min, E_max, num_bins + 1)\n",
    "    bin_centers = 0.5 * (bin_edges[:-1] + bin_edges[1:])\n",
    "    \n",
    "    bin_ipr = np.zeros(num_bins)\n",
    "    bin_counts = np.zeros(num_bins)\n",
    "    \n",
    "    bin_indices = np.digitize(eigval, bin_edges) - 1\n",
    "    bin_indices = np.clip(bin_indices, 0, num_bins - 1)\n",
    "    \n",
    "    for i in range(len(eigval)):\n",
    "        bin_idx = bin_indices[i]\n",
    "        bin_ipr[bin_idx] += ipr[i]\n",
    "        bin_counts[bin_idx] += 1\n",
    "    \n",
    "    bin_ipr = np.divide(bin_ipr, bin_counts, out=np.zeros_like(bin_ipr), where=bin_counts > 0)\n",
    "    \n",
    "    center_idx = num_bins // 2\n",
    "    E_c_lower = E_min\n",
    "    E_c_upper = E_max\n",
    "    \n",
    "    for i in range(center_idx, -1, -1):\n",
    "        if bin_counts[i] > 0 and bin_ipr[i] > ipr_threshold:\n",
    "            E_c_lower = bin_centers[i]\n",
    "            break\n",
    "    \n",
    "    for i in range(center_idx, num_bins):\n",
    "        if bin_counts[i] > 0 and bin_ipr[i] > ipr_threshold:\n",
    "            E_c_upper = bin_centers[i]\n",
    "            break\n",
    "    \n",
    "    return E_c_lower, E_c_upper, bin_centers, bin_ipr\n",
    "\n",
    "\n",
    "def filter_eigenvalues_by_energy(eigval, E_min, E_max):\n",
    "    \"\"\"Filter eigenvalues to those within the energy window.\"\"\"\n",
    "    mask = (eigval >= E_min) & (eigval <= E_max)\n",
    "    return eigval[mask]\n",
    "\n",
    "\n",
    "print(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Figure 1: DOS and IPR Summary\n",
    "\n",
    "2x3 grid showing DOS, eigenvalues, and IPR for a single disorder value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select disorder index and realization\n",
    "disorder_idx = disorder_resolution // 2  # Middle disorder value\n",
    "realization_idx = 0\n",
    "\n",
    "W = disorder_values[disorder_idx]\n",
    "print(f\"Plotting for disorder W={W:.1f}, realization {realization_idx}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what data is available\n",
    "has_H_eigval = 'H_eigval' in data\n",
    "has_SL_eigval = 'SL_eigval' in data\n",
    "has_H_IPR = 'H_IPR' in data\n",
    "has_SL_IPR = 'SL_IPR' in data\n",
    "\n",
    "# Determine grid size\n",
    "n_cols = 2 if (has_H_IPR or has_SL_IPR) else 2\n",
    "if has_H_IPR or has_SL_IPR:\n",
    "    n_cols = 3\n",
    "\n",
    "fig, axs = plt.subplots(2, n_cols, figsize=FIGSIZE_2x3, constrained_layout=True)\n",
    "\n",
    "# Row 0: Hamiltonian\n",
    "if has_H_eigval:\n",
    "    H_eigval = data['H_eigval'][disorder_idx, realization_idx]\n",
    "    \n",
    "    # DOS histogram\n",
    "    axs[0, 0].hist(H_eigval, bins=100, density=True, orientation='horizontal',\n",
    "                   color=COLORS['H'], alpha=0.8)\n",
    "    axs[0, 0].set_title('Hamiltonian DOS', size=TITLE_SIZE)\n",
    "    axs[0, 0].set_xlabel('P(E)', size=LABEL_SIZE)\n",
    "    axs[0, 0].set_ylabel('Energy (E)', size=LABEL_SIZE)\n",
    "    axs[0, 0].grid(True)\n",
    "    \n",
    "    # Eigenvalues vs Index\n",
    "    H_indices = np.arange(len(H_eigval))\n",
    "    axs[0, 1].scatter(H_indices, np.sort(H_eigval), s=1, c=COLORS['H'], alpha=0.5)\n",
    "    axs[0, 1].set_title('Hamiltonian Eigenvalues', size=TITLE_SIZE)\n",
    "    axs[0, 1].set_xlabel('Index', size=LABEL_SIZE)\n",
    "    axs[0, 1].set_ylabel('Energy (E)', size=LABEL_SIZE)\n",
    "    axs[0, 1].grid(True)\n",
    "\n",
    "# Row 0, Col 2: H IPR\n",
    "if has_H_IPR and n_cols > 2:\n",
    "    H_IPR = data['H_IPR'][disorder_idx, realization_idx]\n",
    "    H_sort_idx = np.argsort(data['H_eigval'][disorder_idx, realization_idx]) if has_H_eigval else np.arange(len(H_IPR))\n",
    "    \n",
    "    axs[0, 2].scatter(np.arange(len(H_IPR)), H_IPR[H_sort_idx], s=1, c=COLORS['H'], alpha=0.5)\n",
    "    axs[0, 2].set_title('Hamiltonian IPR', size=TITLE_SIZE)\n",
    "    axs[0, 2].set_xlabel('Index', size=LABEL_SIZE)\n",
    "    axs[0, 2].set_ylabel('IPR', size=LABEL_SIZE)\n",
    "    axs[0, 2].grid(True)\n",
    "    \n",
    "    avg_H_IPR = np.mean(H_IPR)\n",
    "    axs[0, 2].text(0.95, 0.95, f'mean={avg_H_IPR:.4f}', transform=axs[0, 2].transAxes,\n",
    "                   fontsize=14, verticalalignment='top', horizontalalignment='right',\n",
    "                   bbox=ANNOTATION_PROPS)\n",
    "\n",
    "# Row 1: Spectral Localizer\n",
    "if has_SL_eigval:\n",
    "    SL_eigval = data['SL_eigval'][disorder_idx, realization_idx]\n",
    "    \n",
    "    # DOS histogram\n",
    "    axs[1, 0].hist(SL_eigval, bins=100, density=True, orientation='horizontal',\n",
    "                   color=COLORS['SL'], alpha=0.8)\n",
    "    axs[1, 0].set_title('Spectral Localiser DOS', size=TITLE_SIZE)\n",
    "    axs[1, 0].set_xlabel('P(E)', size=LABEL_SIZE)\n",
    "    axs[1, 0].set_ylabel('Eigenvalue', size=LABEL_SIZE)\n",
    "    axs[1, 0].grid(True)\n",
    "    \n",
    "    # Eigenvalues vs Index\n",
    "    SL_indices = np.arange(len(SL_eigval))\n",
    "    axs[1, 1].scatter(SL_indices, np.sort(SL_eigval), s=1, c=COLORS['SL'], alpha=0.5)\n",
    "    axs[1, 1].set_title('Spectral Localiser Eigenvalues', size=TITLE_SIZE)\n",
    "    axs[1, 1].set_xlabel('Index', size=LABEL_SIZE)\n",
    "    axs[1, 1].set_ylabel('Eigenvalue', size=LABEL_SIZE)\n",
    "    axs[1, 1].grid(True)\n",
    "\n",
    "# Row 1, Col 2: SL IPR\n",
    "if has_SL_IPR and n_cols > 2:\n",
    "    SL_IPR = data['SL_IPR'][disorder_idx, realization_idx]\n",
    "    SL_sort_idx = np.argsort(data['SL_eigval'][disorder_idx, realization_idx]) if has_SL_eigval else np.arange(len(SL_IPR))\n",
    "    \n",
    "    axs[1, 2].scatter(np.arange(len(SL_IPR)), SL_IPR[SL_sort_idx], s=1, c=COLORS['SL'], alpha=0.5)\n",
    "    axs[1, 2].set_title('Spectral Localiser IPR', size=TITLE_SIZE)\n",
    "    axs[1, 2].set_xlabel('Index', size=LABEL_SIZE)\n",
    "    axs[1, 2].set_ylabel('IPR', size=LABEL_SIZE)\n",
    "    axs[1, 2].grid(True)\n",
    "    \n",
    "    avg_SL_IPR = np.mean(SL_IPR)\n",
    "    axs[1, 2].text(0.95, 0.95, f'mean={avg_SL_IPR:.4f}', transform=axs[1, 2].transAxes,\n",
    "                   fontsize=14, verticalalignment='top', horizontalalignment='right',\n",
    "                   bbox=ANNOTATION_PROPS)\n",
    "\n",
    "fig.suptitle(f'DOS and IPR at Disorder W={W:.2f}, L={L}', fontsize=SUPTITLE_SIZE)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Figure 2: Energy-Resolved IPR\n",
    "\n",
    "Shows IPR vs Energy to visualize the mobility edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select disorder indices to plot (low, mid, high)\n",
    "disorder_indices = [0, disorder_resolution // 2, disorder_resolution - 1]\n",
    "n_disorders = len(disorder_indices)\n",
    "\n",
    "print(f\"Plotting for disorder values: {[disorder_values[i] for i in disorder_indices]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'H_IPR' not in data and 'SL_IPR' not in data:\n",
    "    print(\"No IPR data available. Skipping this plot.\")\n",
    "else:\n",
    "    fig, axs = plt.subplots(2, n_disorders, figsize=(6*n_disorders, 10), constrained_layout=True)\n",
    "    \n",
    "    if n_disorders == 1:\n",
    "        axs = axs.reshape(2, 1)\n",
    "    \n",
    "    for col, d_idx in enumerate(disorder_indices):\n",
    "        W = disorder_values[d_idx]\n",
    "        \n",
    "        # Aggregate over realizations\n",
    "        if 'H_eigval' in data and 'H_IPR' in data:\n",
    "            H_eigval_all = data['H_eigval'][d_idx, :, :].flatten()\n",
    "            H_IPR_all = data['H_IPR'][d_idx, :, :].flatten()\n",
    "            \n",
    "            # Apply percentile cutoff\n",
    "            H_cutoff = np.percentile(H_IPR_all, IPR_PERCENTILE_CUTOFF)\n",
    "            H_mask = H_IPR_all <= H_cutoff\n",
    "            \n",
    "            print(f\"W={W:.1f} H: IPR range [{H_IPR_all.min():.6f}, {H_IPR_all.max():.6f}], cutoff={H_cutoff:.6f}\")\n",
    "            \n",
    "            axs[0, col].scatter(H_eigval_all[H_mask], H_IPR_all[H_mask], s=0.5, c=COLORS['H'], alpha=0.3)\n",
    "            axs[0, col].set_title(f'H: W={W:.1f}', size=TITLE_SIZE)\n",
    "            axs[0, col].set_xlabel('Energy (E)', size=LABEL_SIZE)\n",
    "            axs[0, col].set_ylabel('IPR', size=LABEL_SIZE)\n",
    "            axs[0, col].grid(True)\n",
    "        \n",
    "        if 'SL_eigval' in data and 'SL_IPR' in data:\n",
    "            SL_eigval_all = data['SL_eigval'][d_idx, :, :].flatten()\n",
    "            SL_IPR_all = data['SL_IPR'][d_idx, :, :].flatten()\n",
    "            \n",
    "            # Apply percentile cutoff\n",
    "            SL_cutoff = np.percentile(SL_IPR_all, IPR_PERCENTILE_CUTOFF)\n",
    "            SL_mask = SL_IPR_all <= SL_cutoff\n",
    "            \n",
    "            print(f\"W={W:.1f} SL: IPR range [{SL_IPR_all.min():.6f}, {SL_IPR_all.max():.6f}], cutoff={SL_cutoff:.6f}\")\n",
    "            \n",
    "            axs[1, col].scatter(SL_eigval_all[SL_mask], SL_IPR_all[SL_mask], s=0.5, c=COLORS['SL'], alpha=0.3)\n",
    "            axs[1, col].set_title(f'SL: W={W:.1f}', size=TITLE_SIZE)\n",
    "            axs[1, col].set_xlabel('Eigenvalue', size=LABEL_SIZE)\n",
    "            axs[1, col].set_ylabel('IPR', size=LABEL_SIZE)\n",
    "            axs[1, col].grid(True)\n",
    "    \n",
    "    fig.suptitle(f'Energy-Resolved IPR (bottom {IPR_PERCENTILE_CUTOFF}%), L={L}', fontsize=SUPTITLE_SIZE)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Figure 3: Mobility Edge Trajectory\n",
    "\n",
    "Shows how the mobility edge changes with disorder strength."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'H_eigval' not in data or 'H_IPR' not in data:\n",
    "    print(\"Need both H_eigval and H_IPR to compute mobility edge. Skipping.\")\n",
    "else:\n",
    "    n_disorder = len(disorder_values)\n",
    "    n_real = data['H_eigval'].shape[1]\n",
    "    \n",
    "    H_Ec_lower = np.zeros((n_disorder, n_real))\n",
    "    H_Ec_upper = np.zeros((n_disorder, n_real))\n",
    "    \n",
    "    for d_idx in range(n_disorder):\n",
    "        for r_idx in range(n_real):\n",
    "            H_eigval = data['H_eigval'][d_idx, r_idx]\n",
    "            H_IPR = data['H_IPR'][d_idx, r_idx]\n",
    "            \n",
    "            Ec_l, Ec_u, _, _ = extract_mobility_edge(H_eigval, H_IPR)\n",
    "            H_Ec_lower[d_idx, r_idx] = Ec_l\n",
    "            H_Ec_upper[d_idx, r_idx] = Ec_u\n",
    "    \n",
    "    # Mean and std across realizations\n",
    "    Ec_lower_mean = H_Ec_lower.mean(axis=1)\n",
    "    Ec_lower_std = H_Ec_lower.std(axis=1)\n",
    "    Ec_upper_mean = H_Ec_upper.mean(axis=1)\n",
    "    Ec_upper_std = H_Ec_upper.std(axis=1)\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 2, figsize=(14, 6), constrained_layout=True)\n",
    "    \n",
    "    axs[0].errorbar(disorder_values, Ec_lower_mean, yerr=Ec_lower_std,\n",
    "                    label=f'L={L}', color=COLORS['H'], marker='o', capsize=3)\n",
    "    axs[0].set_title('Lower Mobility Edge (E < 0)', size=TITLE_SIZE)\n",
    "    axs[0].set_xlabel('Disorder Strength W', size=LABEL_SIZE)\n",
    "    axs[0].set_ylabel('E_c (lower)', size=LABEL_SIZE)\n",
    "    axs[0].legend()\n",
    "    axs[0].grid(True)\n",
    "    \n",
    "    axs[1].errorbar(disorder_values, Ec_upper_mean, yerr=Ec_upper_std,\n",
    "                    label=f'L={L}', color=COLORS['H'], marker='o', capsize=3)\n",
    "    axs[1].set_title('Upper Mobility Edge (E > 0)', size=TITLE_SIZE)\n",
    "    axs[1].set_xlabel('Disorder Strength W', size=LABEL_SIZE)\n",
    "    axs[1].set_ylabel('E_c (upper)', size=LABEL_SIZE)\n",
    "    axs[1].legend()\n",
    "    axs[1].grid(True)\n",
    "    \n",
    "    fig.suptitle('Mobility Edge Trajectory vs Disorder', fontsize=SUPTITLE_SIZE)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Figure 4: Filtered r/z Statistics\n",
    "\n",
    "Spectral statistics filtered by the mobility edge energy window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'H_eigval' not in data or 'H_IPR' not in data:\n",
    "    print(\"Need both H_eigval and H_IPR for filtered statistics. Skipping.\")\n",
    "else:\n",
    "    n_disorder = len(disorder_values)\n",
    "    n_real = data['H_eigval'].shape[1]\n",
    "    \n",
    "    H_r_vals = np.zeros((n_disorder, n_real))\n",
    "    H_z_vals = np.zeros((n_disorder, n_real))\n",
    "    SL_r_vals = np.zeros((n_disorder, n_real)) if 'SL_eigval' in data else None\n",
    "    SL_z_vals = np.zeros((n_disorder, n_real)) if 'SL_eigval' in data else None\n",
    "    \n",
    "    for d_idx in range(n_disorder):\n",
    "        for r_idx in range(n_real):\n",
    "            H_eigval = data['H_eigval'][d_idx, r_idx]\n",
    "            H_IPR = data['H_IPR'][d_idx, r_idx]\n",
    "            \n",
    "            # Extract mobility edge\n",
    "            Ec_l, Ec_u, _, _ = extract_mobility_edge(H_eigval, H_IPR)\n",
    "            \n",
    "            # Filter H eigenvalues\n",
    "            H_filtered = filter_eigenvalues_by_energy(H_eigval, Ec_l, Ec_u)\n",
    "            \n",
    "            if len(H_filtered) > 5:\n",
    "                H_r_vals[d_idx, r_idx] = calculate_r(H_filtered)\n",
    "                H_z_vals[d_idx, r_idx] = calculate_z(H_filtered)\n",
    "            else:\n",
    "                H_r_vals[d_idx, r_idx] = np.nan\n",
    "                H_z_vals[d_idx, r_idx] = np.nan\n",
    "            \n",
    "            # Filter SL eigenvalues using H's mobility edge\n",
    "            if 'SL_eigval' in data:\n",
    "                SL_eigval = data['SL_eigval'][d_idx, r_idx]\n",
    "                SL_filtered = filter_eigenvalues_by_energy(SL_eigval, Ec_l, Ec_u)\n",
    "                \n",
    "                if len(SL_filtered) > 5:\n",
    "                    SL_r_vals[d_idx, r_idx] = calculate_r(SL_filtered)\n",
    "                    SL_z_vals[d_idx, r_idx] = calculate_z(SL_filtered)\n",
    "                else:\n",
    "                    SL_r_vals[d_idx, r_idx] = np.nan\n",
    "                    SL_z_vals[d_idx, r_idx] = np.nan\n",
    "    \n",
    "    # Compute means and standard errors\n",
    "    H_r_mean = np.nanmean(H_r_vals, axis=1)\n",
    "    H_r_std = np.nanstd(H_r_vals, axis=1) / np.sqrt(n_real)\n",
    "    H_z_mean = np.nanmean(H_z_vals, axis=1)\n",
    "    H_z_std = np.nanstd(H_z_vals, axis=1) / np.sqrt(n_real)\n",
    "    \n",
    "    fig, axs = plt.subplots(2, 2, figsize=FIGSIZE_2x2, constrained_layout=True)\n",
    "    \n",
    "    # H r-statistic\n",
    "    axs[0, 0].errorbar(disorder_values, H_r_mean, yerr=H_r_std,\n",
    "                       label=f'L={L}', marker='o', capsize=3, color=COLORS['H'])\n",
    "    axs[0, 0].axhline(y=GOE_R, color='red', linestyle='--', label='GOE', alpha=0.7)\n",
    "    axs[0, 0].axhline(y=POISSON_R, color='green', linestyle='--', label='Poisson', alpha=0.7)\n",
    "    axs[0, 0].set_title('Hamiltonian <r> (filtered)', size=TITLE_SIZE)\n",
    "    axs[0, 0].set_xlabel('Disorder Strength W', size=LABEL_SIZE)\n",
    "    axs[0, 0].set_ylabel('<r>', size=LABEL_SIZE)\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].grid(True)\n",
    "    \n",
    "    # H z-statistic\n",
    "    axs[0, 1].errorbar(disorder_values, H_z_mean, yerr=H_z_std,\n",
    "                       label=f'L={L}', marker='o', capsize=3, color=COLORS['H'])\n",
    "    axs[0, 1].axhline(y=GOE_Z, color='red', linestyle='--', label='GOE', alpha=0.7)\n",
    "    axs[0, 1].axhline(y=POISSON_Z, color='green', linestyle='--', label='Poisson', alpha=0.7)\n",
    "    axs[0, 1].set_title('Hamiltonian <z> (filtered)', size=TITLE_SIZE)\n",
    "    axs[0, 1].set_xlabel('Disorder Strength W', size=LABEL_SIZE)\n",
    "    axs[0, 1].set_ylabel('<z>', size=LABEL_SIZE)\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].grid(True)\n",
    "    \n",
    "    # SL statistics (if available)\n",
    "    if SL_r_vals is not None:\n",
    "        SL_r_mean = np.nanmean(SL_r_vals, axis=1)\n",
    "        SL_r_std = np.nanstd(SL_r_vals, axis=1) / np.sqrt(n_real)\n",
    "        SL_z_mean = np.nanmean(SL_z_vals, axis=1)\n",
    "        SL_z_std = np.nanstd(SL_z_vals, axis=1) / np.sqrt(n_real)\n",
    "        \n",
    "        axs[1, 0].errorbar(disorder_values, SL_r_mean, yerr=SL_r_std,\n",
    "                           label=f'L={L}', marker='o', capsize=3, color=COLORS['SL'])\n",
    "        axs[1, 0].axhline(y=GOE_R, color='red', linestyle='--', label='GOE', alpha=0.7)\n",
    "        axs[1, 0].axhline(y=POISSON_R, color='green', linestyle='--', label='Poisson', alpha=0.7)\n",
    "        axs[1, 0].set_title('Spectral Localizer <r> (filtered)', size=TITLE_SIZE)\n",
    "        axs[1, 0].set_xlabel('Disorder Strength W', size=LABEL_SIZE)\n",
    "        axs[1, 0].set_ylabel('<r>', size=LABEL_SIZE)\n",
    "        axs[1, 0].legend()\n",
    "        axs[1, 0].grid(True)\n",
    "        \n",
    "        axs[1, 1].errorbar(disorder_values, SL_z_mean, yerr=SL_z_std,\n",
    "                           label=f'L={L}', marker='o', capsize=3, color=COLORS['SL'])\n",
    "        axs[1, 1].axhline(y=GOE_Z, color='red', linestyle='--', label='GOE', alpha=0.7)\n",
    "        axs[1, 1].axhline(y=POISSON_Z, color='green', linestyle='--', label='Poisson', alpha=0.7)\n",
    "        axs[1, 1].set_title('Spectral Localizer <z> (filtered)', size=TITLE_SIZE)\n",
    "        axs[1, 1].set_xlabel('Disorder Strength W', size=LABEL_SIZE)\n",
    "        axs[1, 1].set_ylabel('<z>', size=LABEL_SIZE)\n",
    "        axs[1, 1].legend()\n",
    "        axs[1, 1].grid(True)\n",
    "    \n",
    "    fig.suptitle(f'Filtered Spectral Statistics (|E| < E_c), L={L}', fontsize=SUPTITLE_SIZE)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
